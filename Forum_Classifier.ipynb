{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forum_Classifier",
      "provenance": [],
      "authorship_tag": "ABX9TyO8IfesXCDgORjNv8ZzmC/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phillipchau/BERT-Forum-Classifier/blob/master/Forum_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-5vMqGglrFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "ff63a9cc-55c9-4292-eb1f-a4774cefaf76"
      },
      "source": [
        "!pip install transformers torch pandas gspread gspread-dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 6.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 7.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: gspread-dataframe in /usr/local/lib/python3.6/dist-packages (3.0.7)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a02e14b008edd97a981bcd1a459be4def4a6049c620dd621347558cda342db44\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovzq-tzPlthh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8a35626e-2466-493e-c4cd-619fe3bb1d13"
      },
      "source": [
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "default_app = GoogleCredentials.get_application_default()\n",
        "gc = gspread.authorize(default_app)\n",
        "# Display full col_width\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "SPREADSHEET_NAMES = ['huel_forum_posts', \n",
        "                     'hopscotch_forum_posts',\n",
        "                     'bnz_forum_posts', \n",
        "                     'schizophrenia_forum_posts']\n",
        "\n",
        "def loadgsheet(spreadsheet_name):\n",
        "  spreadsheet = gc.open(spreadsheet_name)\n",
        "  worksheet = spreadsheet.worksheet(title=spreadsheet_name)\n",
        "  columns=['post_text', 'post_id', 'user_id',\t'username',\t'topic_id',\t'post_num',\t'reply_count',\t'created_at',\t'updated_at',\t'num_reads',\t'topic_slug',\t'forum_name']\n",
        "  return get_as_dataframe(worksheet)[:2000][columns]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooQO_5sblwX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1cf7314-f709-4b27-e71f-fb69edf1f9c4"
      },
      "source": [
        "df_all = pd.DataFrame()\n",
        "for sheet_name in SPREADSHEET_NAMES:\n",
        "  print(f'Joining {sheet_name}')\n",
        "  df = loadgsheet(sheet_name)\n",
        "  print(df.head(2))\n",
        "  df_all = pd.concat([df_all, df]).dropna(axis=0)\n",
        "\n",
        "df_all.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Joining huel_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       post_text  ...  forum_name\n",
            "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)   ...  huel      \n",
            "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...  huel      \n",
            "\n",
            "[2 rows x 12 columns]\n",
            "Joining hopscotch_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               post_text  ... forum_name\n",
            "0  Anything about the Hopscotch WebPlayer? Talk about that here! creationsofanoob\">CreationsOfaNoob\\nThis can include possibly modding the webplayer or a certain part of how it works in general.\\nI made a small tweak to “fix” line spacing issues with the “copied glitches” project, but I know very little about how the whole WebPlayer works.\\nTell me if I am wrong, but I think I am the first here to mod the WebPlayer.\\nHow does it relate to HS?\\nWe can see how the webplayer can improve by analyzing and tweaking small parts about it.  ...  hopscotch\n",
            "1  This is a great idea! My iPad is a “family iPad” so I don’t want to change any code and JSONs because of that and also because that I don’t want my Hopscotch App to stop working. But I am interested in how the web player works? Does it have to be modified for each project or can you just modify a single file or directory? And how does these changes take effect? Do you just have to publish a project with a modified web player for it to work?                                                                                           ...  hopscotch\n",
            "\n",
            "[2 rows x 12 columns]\n",
            "Joining bnz_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                post_text  ...  forum_name\n",
            "0  Today is **‘Safer Internet Day’** , which is celebrated globally in February each year. It’s a day dedicated to promoting a safer online world. Netsafe are the official organising committee for this in New Zealand, and BNZ are proud to support them to encourage conversations about what a safer internet could look like. At BNZ, we think it’s an important day to recognise given that if you’re not staying safe online, you’re more likely to become a target of financial crime.     Netsafe have shared some great tips and resources, including online safety conversation starters for kids (under ten year olds), tweens (10-13 year olds) and teens (14-18 year olds). They recommend having regular chats with your young ones to educate them about online safety, and ultimately help minimise the damage if things do go wrong online. The types of questions they suggest asking your children are:  * What are some things you shouldn’t share online?  * What is the difference between public and private posts?  * Do you know what privacy settings are and what they are used for?    On top of this, Netsafe have a ‘Parents Toolkit’, as well as guides for checking privacy settings for some of the major social media channels – Facebook, Twitter, Instagram and Snapchat.     To read these resources in detail, and even print them off to take home so you can ask your kids, visit the following page <https://www.netsafe.org.nz/safer- internet-day/>     [![FB & IG Proudly supporting 2](https://aws1.discourse- cdn.com/bnz/optimized/2X/d/daa286b1727bf23324a80964f2bcf67079936775_2_500x500.jpeg) FB &amp; IG Proudly supporting 21200×1200 519 KB ](https://aws1.discourse- cdn.com/bnz/original/2X/d/daa286b1727bf23324a80964f2bcf67079936775.jpeg \"FB &amp; IG Proudly supporting 2\")   ...  bnz       \n",
            "1  When I go online to set a end date on a automatic payment it will not allow a date to be entered forcing me to call to get it done.  I was told it’s a known issue but is there any ideas on when the issue will be fixed, it’s been months and months since the new internet banking was rolled out and it’s never worked on this system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  bnz       \n",
            "\n",
            "[2 rows x 12 columns]\n",
            "Joining schizophrenia_forum_posts\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             post_text  ...     forum_name\n",
            "0  ![](http://www.peteearley.com/wp-content/themes/education/images/favicon.ico) [Pete Earley – 8 Jun 20](http://www.peteearley.com/2020/06/08/federal-govt- accused-of-abandoning-research-that-would-provide-short-term-help-to-the-most- seriously-mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29 \"12:00PM - 08 June 2020\") ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/2/f2e8cca16cf15cebbe4ee807f64c29a9dcc09d25.jpeg) ### [Federal Govt. Accused Of Abandoning Research That Would Provide Short Term...](http://www.peteearley.com/2020/06/08/federal-govt-accused-of- abandoning-research-that-would-provide-short-term-help-to-the-most-seriously- mentally- ill/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A%2Bpeteearley%2B%28The%2BOfficial%2BBlog%2Bof%2BAuthor%2BPete%2BEarley%29) Dr. E. Fuller Torrey rips into NIMH, its advisory board and NAMI (6-8-20) Dr. E. Fuller Torrey is again accusing the National Institutes of Mental Health of virtually abandoning clinical trials that could help Americans with schizophrenia and bipolar...   ...  schizophrenia\n",
            "1  I’m taking my first capsule of Lumateperone 42 mg tonight. Steady state should be reached in 5 days. Will share my experiences here.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ...  schizophrenia\n",
            "\n",
            "[2 rows x 12 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)</td>\n",
              "      <td>25656</td>\n",
              "      <td>4859</td>\n",
              "      <td>Justin_Keikhlasan</td>\n",
              "      <td>8815.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-03T17:57:25.143Z</td>\n",
              "      <td>2020-06-08T20:27:53.011Z</td>\n",
              "      <td>18.0</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng</td>\n",
              "      <td>25662</td>\n",
              "      <td>4856</td>\n",
              "      <td>matt009</td>\n",
              "      <td>8815.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2020-06-04T03:20:52.395Z</td>\n",
              "      <td>2020-06-04T03:20:52.395Z</td>\n",
              "      <td>15.0</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.</td>\n",
              "      <td>25677</td>\n",
              "      <td>4635</td>\n",
              "      <td>Tom</td>\n",
              "      <td>8815.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2020-06-04T17:03:37.547Z</td>\n",
              "      <td>2020-06-04T17:04:06.818Z</td>\n",
              "      <td>14.0</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i tried choc cherry only one i didnt like</td>\n",
              "      <td>25678</td>\n",
              "      <td>4856</td>\n",
              "      <td>matt009</td>\n",
              "      <td>8815.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>2020-06-04T18:19:22.386Z</td>\n",
              "      <td>14.0</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>**Mint Chocolate** \\- 5/5. Like [@Tom](/u/tom) said, we think it tastes just like you would expect. It’s my wife’s favorite way to have Huel, and also her favorite Soylent RTD flavor, which is more like a treat than a meal.  **Peanut Butter** \\- 3/5. It’s decent, but I’m pretty sure it has the same calories as peanut butter powders like PBFit and PB2, which taste more like peanut butter.  **Gingerbread** \\- 3/5. Not bad, but I don’t know that “Gingerbread” is an accurate name for the flavor. I made my own by combining the spices from a gingerbread recipe into a jar. (allspice, cinnamon, ginger, cloves, nutmeg).  **Salted Caramel** \\- 2/5. Only tried it once from the sample pack. I understand that “salted” is in the name, but I felt as if the salt flavor was too prominent. Like when too much salt from the rim falls into your magarita, and end up with margarita-flavored sea water.  **Banana** \\- If you like banana flavored stuff (candy / taffy) then you’ll most likely enjoy this. I like it just fine, and my wife wouldn’t even try it after smelling it. [@Justin_Keikhlasan](/u/justin_keikhlasan) I’ve always wondered about the Vanilla flavor boost. How did you use it? What flavor of Huel did you add it to? How did the flavor compare to Vanilla Huel?</td>\n",
              "      <td>25679</td>\n",
              "      <td>4073</td>\n",
              "      <td>ericb</td>\n",
              "      <td>8815.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2020-06-04T19:57:36.200Z</td>\n",
              "      <td>2020-06-04T20:01:21.397Z</td>\n",
              "      <td>13.0</td>\n",
              "      <td>flavor-booster-ratings</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            post_text  ... forum_name\n",
              "0  Purchased the trial pack, and used all flavors combining with Chocolate Huel Black Edition. Anyone that tried Berry with the Vanilla Huel? Share your ratings, vote your top 5 favorites ![:grin:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/grin.png?v=9)  * Vanilla  * Salted Caramel  * Banana  * Strawberry  * Apple Cinnamon  * Chocolate  * Peanut Butter  * Gingerbread  * Mocha  * Chocolate Cherry  * Mint-Chocolate  * Pumpkin Spice  * Berry 0 voters **Apple Cinnamon** \\- Excellent (5/5)  **Chocolate** \\- Double chocolate (5/5  **Pumpkin Spice** \\- Very Good (4/5)  **Peanut Butter** \\- Very Good (4/5)  **Vanilla** \\- Very Good (4/5)  **Mint Chocolate** \\- Not bad (3/5)  **Salted Caramel** \\- Not Bad (3/5)  **Mocha** \\- Not much of a difference to me (3/5)  **Strawberry** \\- Not Bad (2/5)  **Banana** \\- Not Bad (2/5)  **Berry Flavor** \\- Horrible(1/5)                                                                                                                                                                                                                                                                                                                                                                                                                                        ...  huel     \n",
              "1  i like mocha the best but it fks wth my stomach acid ![:face_with_hand_over_mouth:](https://sjc2.discourse- cdn.com/standard14/images/emoji/apple/face_with_hand_over_mouth.png?v=9) berry and banana i like too apple cinamon amazng                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...  huel     \n",
              "2   Interesting! Here are my ratings (I haven’t tried them all yet). **Mint Chocolate** \\- 5/5. To me this tastes exactly like what you’d expect a good chocolate mint to taste like.  **Chocolate Cherry** \\- 4.5/5. I end up using a little more than the serving size because I always want it to have more cherry flavor. Otherwise, really good.  **Apple Cinnamon** \\- 4/5. To me this taste like apple pie, which is great. I could rate it 5/5, except that because it’s so sweet, I feel like I need to cycle it with other flavors. If this was my only flavor, I’d probably get sick of it.  **Gingerbread** \\- 3/5. It’s another one I wish was a little stronger. I’ve always liked gingerbread a little more gingery too, but I’ve enjoyed trying this out and mixing it into the rotation.  **Salted Caramel** \\- 3/5. It’s totally fine, but this was just never going to be my favorite flavor. Also, I think I’ve realized I’d rather get the extra salt somewhere else in my diet. I would expect people who like salted caramel to mostly be happy with this flavor.  **Berry** \\- 2.5/5. To me it tastes more like a sugary kids cereal kind of berry than actual berries. I add a couple drops of orange extract every time now, and it tastes much more like actual fruit (unsurprisingly). I’d probably rate that combo 4/5.   ...  huel     \n",
              "3  i tried choc cherry only one i didnt like                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  huel     \n",
              "4    **Mint Chocolate** \\- 5/5. Like [@Tom](/u/tom) said, we think it tastes just like you would expect. It’s my wife’s favorite way to have Huel, and also her favorite Soylent RTD flavor, which is more like a treat than a meal.  **Peanut Butter** \\- 3/5. It’s decent, but I’m pretty sure it has the same calories as peanut butter powders like PBFit and PB2, which taste more like peanut butter.  **Gingerbread** \\- 3/5. Not bad, but I don’t know that “Gingerbread” is an accurate name for the flavor. I made my own by combining the spices from a gingerbread recipe into a jar. (allspice, cinnamon, ginger, cloves, nutmeg).  **Salted Caramel** \\- 2/5. Only tried it once from the sample pack. I understand that “salted” is in the name, but I felt as if the salt flavor was too prominent. Like when too much salt from the rim falls into your magarita, and end up with margarita-flavored sea water.  **Banana** \\- If you like banana flavored stuff (candy / taffy) then you’ll most likely enjoy this. I like it just fine, and my wife wouldn’t even try it after smelling it. [@Justin_Keikhlasan](/u/justin_keikhlasan) I’ve always wondered about the Vanilla flavor boost. How did you use it? What flavor of Huel did you add it to? How did the flavor compare to Vanilla Huel?                                  ...  huel     \n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cDRXhji9PVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "dcd3685c-80b0-4e98-a700-4c60a135e637"
      },
      "source": [
        "# Shuffle df\n",
        "df_all = df_all.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sample df\n",
        "df_all[['post_text', 'forum_name']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>forum_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3036</th>\n",
              "      <td>Thanks guys [@Pianogal](/u/pianogal) ![:slight_smile:](https://sjc5.discourse- cdn.com/schizophrenia/images/emoji/twitter/slight_smile.png?v=9)</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3596</th>\n",
              "      <td>Happy cake day [@Mountainman](/u/mountainman)!</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1653</th>\n",
              "      <td>[Medscape](https://www.medscape.com/viewarticle/931832) ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/8/f89e4c9a8bc1d737a7a363b1dadc7db6c14267fa.jpeg) ### [Antipsychotic May Fill Schizophrenia Treatment Need](https://www.medscape.com/viewarticle/931832) Adjunctive treatment with the atypical antipsychotic pimavanserin is associated with marked improvement in patients with schizophrenia who have predominantly negative symptoms, new research suggests.</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5982</th>\n",
              "      <td>Only Gluten free chocolate in stock. Will Huel honor $59 gluten price if I reorder via Gluten free?</td>\n",
              "      <td>huel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3720</th>\n",
              "      <td>Beautiful! Reminds me of my childhood in Maryland on the water and near a lake by my house ![:slightly_smiling_face:](https://sjc5.discourse- cdn.com/schizophrenia/images/emoji/twitter/slightly_smiling_face.png?v=9)</td>\n",
              "      <td>schizophrenia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     post_text     forum_name\n",
              "3036  Thanks guys [@Pianogal](/u/pianogal) ![:slight_smile:](https://sjc5.discourse- cdn.com/schizophrenia/images/emoji/twitter/slight_smile.png?v=9)                                                                                                                                                                                                                                                                                                                                                           schizophrenia\n",
              "3596  Happy cake day [@Mountainman](/u/mountainman)!                                                                                                                                                                                                                                                                                                                                                                                                                                                            schizophrenia\n",
              "1653  [Medscape](https://www.medscape.com/viewarticle/931832) ![](https://aws1.discourse- cdn.com/schizophrenia/uploads/default/original/3X/f/8/f89e4c9a8bc1d737a7a363b1dadc7db6c14267fa.jpeg) ### [Antipsychotic May Fill Schizophrenia Treatment Need](https://www.medscape.com/viewarticle/931832) Adjunctive treatment with the atypical antipsychotic pimavanserin is associated with marked improvement in patients with schizophrenia who have predominantly negative symptoms, new research suggests.   schizophrenia\n",
              "5982  Only Gluten free chocolate in stock. Will Huel honor $59 gluten price if I reorder via Gluten free?                                                                                                                                                                                                                                                                                                                                                                                                       huel         \n",
              "3720  Beautiful! Reminds me of my childhood in Maryland on the water and near a lake by my house ![:slightly_smiling_face:](https://sjc5.discourse- cdn.com/schizophrenia/images/emoji/twitter/slightly_smiling_face.png?v=9)                                                                                                                                                                                                                                                                                   schizophrenia"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW5wWd4t9Pf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "032ce41a-ef58-4db6-df1a-033f21eb4a09"
      },
      "source": [
        "df_all.isnull().sum()  # No nulls as we dropna on the fly when loading"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_text      0\n",
              "post_id        0\n",
              "user_id        0\n",
              "username       0\n",
              "topic_id       0\n",
              "post_num       0\n",
              "reply_count    0\n",
              "created_at     0\n",
              "updated_at     0\n",
              "num_reads      0\n",
              "topic_slug     0\n",
              "forum_name     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B083JkRj9d_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all = df_all.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlV3hs-S9gxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "f021e4b2-e252-40fc-8943-eb81800d4967"
      },
      "source": [
        "df_all['forum_name'] = df_all['forum_name'].astype('category')\n",
        "df_all['forum_name_encoded'] = df_all['forum_name'].cat.codes.astype('int32')\n",
        "df_all.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4048</th>\n",
              "      <td>Do it yourself in Internet Banking! 1. From the main menu select “Cards” 2. Drag and drop the account you want to access to the “Cheque” or “Savings” slots of your card Delinking is just as easy – just drag and drop the account off the card. Internet Banking will even warn you about possible fees if you try to attach a Rapid Save account to your card. A nice touch to prevent fees. [![Card Management](https://aws1.discourse- cdn.com/bnz/optimized/1X/574ba457d5518b82ec07f94df07e77f2e3d11233_2_690x336.jpg) Card Management.jpg1909×932 182 KB ](https://aws1.discourse- cdn.com/bnz/original/1X/574ba457d5518b82ec07f94df07e77f2e3d11233.jpg \"Card Management.jpg\") #tuesdaytipsfrommegan</td>\n",
              "      <td>32171</td>\n",
              "      <td>731</td>\n",
              "      <td>megan</td>\n",
              "      <td>28451.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2017-08-01T01:38:08.435Z</td>\n",
              "      <td>2019-05-22T04:34:07.310Z</td>\n",
              "      <td>35.0</td>\n",
              "      <td>want-to-swap-the-accounts-that-you-can-access-via-eftpos-or-atm-machine-on-your-card</td>\n",
              "      <td>bnz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>Don’t worry about me - I’ve given up waiting as it’s pretty clear that an inbuilt budgeting / track spending or integration with other accounting and budgeting solutions must be towards the bottom of the BNZ’s priority list. If you want this type of facility you’d be better off with another bank.</td>\n",
              "      <td>34980</td>\n",
              "      <td>2521</td>\n",
              "      <td>Darren</td>\n",
              "      <td>25191.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2018-08-01T23:55:10.056Z</td>\n",
              "      <td>2018-08-01T23:55:10.056Z</td>\n",
              "      <td>30.0</td>\n",
              "      <td>track-spending-and-budgeting-apps</td>\n",
              "      <td>bnz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4693</th>\n",
              "      <td>My old car went to the salvage yard May 8th. Having to live without a car.</td>\n",
              "      <td>2.24231e+06</td>\n",
              "      <td>3408</td>\n",
              "      <td>mike1</td>\n",
              "      <td>199992.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2020-06-09T00:16:56.557Z</td>\n",
              "      <td>2020-06-09T00:16:56.557Z</td>\n",
              "      <td>30.0</td>\n",
              "      <td>i-feel-lonely-anyone-want-a-chat</td>\n",
              "      <td>schizophrenia</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>I definitely try to start buying early - especially with two kids birthdays close to xmas. That goes for presents as well as food. I also like to ‘cash in’ extras around this time - any survey cash, power company points (brownie points - genesis), credit card points and fly buys. It makes them more like saving funds that you don’t even know are happening!</td>\n",
              "      <td>804</td>\n",
              "      <td>269</td>\n",
              "      <td>AC1</td>\n",
              "      <td>93.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2014-11-18T08:43:12.000Z</td>\n",
              "      <td>2016-05-30T09:50:44.288Z</td>\n",
              "      <td>4.0</td>\n",
              "      <td>xmas-spending</td>\n",
              "      <td>bnz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6365</th>\n",
              "      <td>Yeah Adds are pesky and annoying for the user, and despite the profits earned for THT, there are still fees and other financial “difficulties” that go along with them. It might also discourage users from downloading Hopscotch because of the adds. That’s a common theme in the comments of some mobile games on the App Store.</td>\n",
              "      <td>2565099</td>\n",
              "      <td>9301</td>\n",
              "      <td>GweTV</td>\n",
              "      <td>54431.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2020-02-17T16:43:40.624Z</td>\n",
              "      <td>2020-02-17T16:45:43.958Z</td>\n",
              "      <td>48.0</td>\n",
              "      <td>how-hopscotch-can-make-lots-of-money-without-the-sadness</td>\n",
              "      <td>hopscotch</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         post_text  ... forum_name_encoded\n",
              "4048  Do it yourself in Internet Banking! 1. From the main menu select “Cards” 2. Drag and drop the account you want to access to the “Cheque” or “Savings” slots of your card Delinking is just as easy – just drag and drop the account off the card. Internet Banking will even warn you about possible fees if you try to attach a Rapid Save account to your card. A nice touch to prevent fees. [![Card Management](https://aws1.discourse- cdn.com/bnz/optimized/1X/574ba457d5518b82ec07f94df07e77f2e3d11233_2_690x336.jpg) Card Management.jpg1909×932 182 KB ](https://aws1.discourse- cdn.com/bnz/original/1X/574ba457d5518b82ec07f94df07e77f2e3d11233.jpg \"Card Management.jpg\") #tuesdaytipsfrommegan   ...  0                \n",
              "263   Don’t worry about me - I’ve given up waiting as it’s pretty clear that an inbuilt budgeting / track spending or integration with other accounting and budgeting solutions must be towards the bottom of the BNZ’s priority list. If you want this type of facility you’d be better off with another bank.                                                                                                                                                                                                                                                                                                                                                                                                     ...  0                \n",
              "4693  My old car went to the salvage yard May 8th. Having to live without a car.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...  3                \n",
              "1511  I definitely try to start buying early - especially with two kids birthdays close to xmas. That goes for presents as well as food. I also like to ‘cash in’ extras around this time - any survey cash, power company points (brownie points - genesis), credit card points and fly buys. It makes them more like saving funds that you don’t even know are happening!                                                                                                                                                                                                                                                                                                                                         ...  0                \n",
              "6365  Yeah Adds are pesky and annoying for the user, and despite the profits earned for THT, there are still fees and other financial “difficulties” that go along with them. It might also discourage users from downloading Hopscotch because of the adds. That’s a common theme in the comments of some mobile games on the App Store.                                                                                                                                                                                                                                                                                                                                                                           ...  1                \n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43NWiA3P9nh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb24bc39-307b-490e-d70c-c6f665c16e3e"
      },
      "source": [
        "!pip install html2text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: html2text in /usr/local/lib/python3.6/dist-packages (2020.1.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ3aU6i39quv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23e25895-aef6-4946-b5fb-7a987ebe67a8"
      },
      "source": [
        "#filter out HTML\n",
        "import html2text\n",
        "h = html2text.HTML2Text()\n",
        "h.unicode_snob = True\n",
        "df_all['post_text'] = df_all['post_text'].apply((lambda x: h.handle(x).replace('\\n', ' ').replace('  ', ' ')))\n",
        "#filter out urls\n",
        "import re\n",
        "df_all['post_text'] = df_all['post_text'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(x), flags=re.MULTILINE))\n",
        "# Filter emojis\n",
        "!pip install emoji\n",
        "\n",
        "import emoji\n",
        "\n",
        "def give_emoji_free_text(text):\n",
        "    allchars = [str for str in text]\n",
        "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "    return clean_text\n",
        "\n",
        "text = give_emoji_free_text(df_all['post_text'][0])\n",
        "print(text)\n",
        "\n",
        "df_all['post_text'] = df_all['post_text'].apply((lambda x: give_emoji_free_text(x)))\n",
        "df_all.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=61bd5308f714cb8931d4c1c7083895486307d044022c06a98760ff63100c3c07\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Living life without the expectations of others. If your happy you’ve a successful life.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_text</th>\n",
              "      <th>post_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>username</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>post_num</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>updated_at</th>\n",
              "      <th>num_reads</th>\n",
              "      <th>topic_slug</th>\n",
              "      <th>forum_name</th>\n",
              "      <th>forum_name_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4896</th>\n",
              "      <td>It would be cool if Huel added a mint choco RTD. I’d even be ok if it replaced regular choco.</td>\n",
              "      <td>25535</td>\n",
              "      <td>4697</td>\n",
              "      <td>bunkscene</td>\n",
              "      <td>8759.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-05-22T15:51:53.330Z</td>\n",
              "      <td>2020-05-22T15:51:53.330Z</td>\n",
              "      <td>14.0</td>\n",
              "      <td>big-fan-of-new-rtd-vanilla</td>\n",
              "      <td>huel</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5828</th>\n",
              "      <td>The poll was accidentally anonymous XD I fixed it lol ppl said \"YES :D\" for no reason XD</td>\n",
              "      <td>498835</td>\n",
              "      <td>2418</td>\n",
              "      <td>PixelMaster64</td>\n",
              "      <td>19943.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-07-19T20:36:39.129Z</td>\n",
              "      <td>2016-07-19T20:36:39.129Z</td>\n",
              "      <td>48.0</td>\n",
              "      <td>the-ultimate-pixel-art-guide-d</td>\n",
              "      <td>hopscotch</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4892</th>\n",
              "      <td>This topic is no longer a banner. It will no longer appear at the top of every page.</td>\n",
              "      <td>6751</td>\n",
              "      <td>1</td>\n",
              "      <td>Yvonne</td>\n",
              "      <td>1916.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-06-28T22:54:01.006Z</td>\n",
              "      <td>2016-06-28T22:54:01.006Z</td>\n",
              "      <td>5.0</td>\n",
              "      <td>new-site-existing-members-reset-your-password</td>\n",
              "      <td>bnz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5953</th>\n",
              "      <td>Your suggestion about wiping account fees based on deposits is common with Australian banks. NAB (National Bank of Australia who own BNZ), where I have an account, is literally fee free for personal accounts, with no minimum deposit requirement. Even the Visa Debit card is free. (1) On the flipside I also have a business account with BNZ which only costs $5 per month, which is cheaper then what a business account would cost at NAB or any of the Australian banks. So all in all, I am am quite happy with the $10 combined charge per month. I could get a “free” account at Kiwibank but I am quite happy to pay the $60 per year to have free counter transactions, shorter/no queues and not be pushed the marketing message that patriotism is the main reason to be a client all the while being charged among the highest fees. 1.</td>\n",
              "      <td>1019</td>\n",
              "      <td>1027</td>\n",
              "      <td>michaelnz</td>\n",
              "      <td>172.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-01-07T06:22:57.000Z</td>\n",
              "      <td>2016-05-30T09:50:47.158Z</td>\n",
              "      <td>6.0</td>\n",
              "      <td>monthly-account-fee</td>\n",
              "      <td>bnz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>I succeded at two things only: Accept taking my meds Stop thinking about suicide</td>\n",
              "      <td>2.24077e+06</td>\n",
              "      <td>13999</td>\n",
              "      <td>Aziz</td>\n",
              "      <td>199858.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2020-06-07T19:07:33.915Z</td>\n",
              "      <td>2020-06-07T19:07:33.915Z</td>\n",
              "      <td>84.0</td>\n",
              "      <td>how-many-of-you-have-failed-at-everything-you-ever-tried</td>\n",
              "      <td>schizophrenia</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      post_text  ... forum_name_encoded\n",
              "4896  It would be cool if Huel added a mint choco RTD. I’d even be ok if it replaced regular choco.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...  2                \n",
              "5828  The poll was accidentally anonymous XD I fixed it lol ppl said \"YES :D\" for no reason XD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  1                \n",
              "4892  This topic is no longer a banner. It will no longer appear at the top of every page.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...  0                \n",
              "5953  Your suggestion about wiping account fees based on deposits is common with Australian banks. NAB (National Bank of Australia who own BNZ), where I have an account, is literally fee free for personal accounts, with no minimum deposit requirement. Even the Visa Debit card is free. (1) On the flipside I also have a business account with BNZ which only costs $5 per month, which is cheaper then what a business account would cost at NAB or any of the Australian banks. So all in all, I am am quite happy with the $10 combined charge per month. I could get a “free” account at Kiwibank but I am quite happy to pay the $60 per year to have free counter transactions, shorter/no queues and not be pushed the marketing message that patriotism is the main reason to be a client all the while being charged among the highest fees. 1.  ...  0                \n",
              "1119  I succeded at two things only: Accept taking my meds Stop thinking about suicide                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  3                \n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w4N_Eur-DsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "48746dc5-17ce-42a4-c281-bc7e22dc9f41"
      },
      "source": [
        "!pip install transformers torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6EPTqZS_pOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "150176a0-bd2b-46d1-d8bf-2d485596dce1"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHPxnFja_upM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8031d0d3-d271-402d-fb52-0fc6899b99bf"
      },
      "source": [
        "sentences = df_all.post_text.values\n",
        "labels = df_all.forum_name_encoded.values\n",
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Living life without the expectations of others. If your happy you’ve a successful life.\n",
            "Tokenized:  ['living', 'life', 'without', 'the', 'expectations', 'of', 'others', '.', 'if', 'your', 'happy', 'you', '’', 've', 'a', 'successful', 'life', '.']\n",
            "Token IDs:  [2542, 2166, 2302, 1996, 10908, 1997, 2500, 1012, 2065, 2115, 3407, 2017, 1521, 2310, 1037, 3144, 2166, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko8T3rGv_y-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "21914a32-73ab-4482-9c0a-911fe3b76ffb"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    sent = sent[:512]\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Living life without the expectations of others. If your happy you’ve a successful life.\n",
            "Token IDs: [101, 2542, 2166, 2302, 1996, 10908, 1997, 2500, 1012, 2065, 2115, 3407, 2017, 1521, 2310, 1037, 3144, 2166, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfK3OexF_-QL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69695b6b-8b31-48a5-e420-53976e6006d0"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTx-4shmAM5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b34f785-955e-4db5-8797-c0f4c2450b0b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojjpsbdfAVQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f80d38c4-b814-4e0d-f87d-364eba0e9694"
      },
      "source": [
        "!pip install tensorflow==2.2.0\n",
        "!pip install keras\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.2.0 in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.2.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.30.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.0.post3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbUdVZUeAy2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c303117c-94ec-449b-a8c4-77f92f3f2c21"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 62\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 62 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAxPuFrsD2DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUypzhYWEAUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLtvRSZnEDB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAsUFF0-EGYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTD1DFFqEMCz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "450ac7ce-6b88-4688-88ef-29dc1e100e1a"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcxyI37BEWlP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "35e57f8b-e098-4860-b335-1826384c8ef9"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (4, 768)\n",
            "classifier.bias                                                 (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duvAWCVeEYLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zglY8tU1EauT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XK0X-GzEduu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr60D8BSEfYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeF4hmhJEgbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "31343e5a-c2f3-4a48-8b91-f31b78f5c735"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device, dtype=torch.int64)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    224.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    224.    Elapsed: 0:00:54.\n",
            "  Batch   120  of    224.    Elapsed: 0:01:21.\n",
            "  Batch   160  of    224.    Elapsed: 0:01:48.\n",
            "  Batch   200  of    224.    Elapsed: 0:02:15.\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:02:30\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    224.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    224.    Elapsed: 0:00:54.\n",
            "  Batch   120  of    224.    Elapsed: 0:01:21.\n",
            "  Batch   160  of    224.    Elapsed: 0:01:47.\n",
            "  Batch   200  of    224.    Elapsed: 0:02:14.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:02:30\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.91\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch    40  of    224.    Elapsed: 0:00:27.\n",
            "  Batch    80  of    224.    Elapsed: 0:00:54.\n",
            "  Batch   120  of    224.    Elapsed: 0:01:21.\n",
            "  Batch   160  of    224.    Elapsed: 0:01:47.\n",
            "  Batch   200  of    224.    Elapsed: 0:02:14.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:02:30\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}